#Scoring_impact_carbone

# L'idée est de connaitre notre impact carbone personnalisé

Notre objectif
Création d'une application Scoring Impact Carbone sur Streamlit
un outil de calcul de notre empreinte carbone avec un système de recommandation et une visualisation comparative avec des habitants d'autres pays.

Pourquoi ce projet ?
On avait à coeur d'utiliser l'IA et la data science pour un projet orienté goodtech.
Le changement climatique est aujourd'hui notre combat et nous voulions y contribuer à notre manière. 

Comment ?
On nous dit souvent qu'il faut réduire notre impact carbone. 
Oui c'est bien ! Mais quel est notre point de départ ?
C'est en prenant conscience de notre impact carbone que l'on peut agir. 

La promesse :
Agissons grâce à Scoring Impact Carbone !

Créer un impact positif sur nos habitudes grâce à l'IA. 

Exemple d'application de la data :
Aujourd'hui la data est parmi nous, partout.
Prenons exemple sur une montre connectée pour le running. 
Quand vous courrez, si vous êtes dans le rouge, que faites-vous ? Vous ralentissez. 
Si vous êtes à 500m de la ligne d'arrivée et que vous avez du jus, vous pouvez accélerer pour améliorer votre temps. 

## Déroulement du projet

Notre dataset provient de Kaggle, lieu url ci-dessous :
'https://www.kaggle.com/datasets/dumanmesut/individual-carbon-footprint-calculation/data'

1) La préparation du dataset 
Dans notre quotidien de data scientist, nous devons réduire la complexité de notre dataset en réduisant le nombre de colonnes appelées "features". 
Cependant dans notre démarche de scoring, nous devions garder cette compléxité car elle est réalité du quotidien. 
Chaque geste compte dans le scoring de notre impact carbone. 

Notre dataset de base avait 10 000 lignes et 18 features.

En observant notre dataset, nous avons du préparer ce dernier pour l'appliquer à un modèle de machine learning. 
De 18 features nous sommes à 53 après le preprocessing.

2) Le modèle de machine learning
Nous avons essayé plusieurs modèles car notre dataset est relativement complexe car il contient beaucoup de features. 
Nous avons choisi comme modèle le Gradient Boosting Machines(GBM).

Il s’agit d’une autre méthode d’apprentissage par ensemble qui construit plusieurs arbres de décision selon une approche par étapes. 
Il corrige les erreurs faites par les prédicteurs précédents.

C'est le modèle avait les meilleures performances pour notre dataset. 

3) L'interface utilisateur

4) Le système de recommandation 

## Comment améliorer notre projet ?

## Que peut-on rajouter dans notre projet ?

## Tu veux participer notre projet ? 
### Procédure


## Une équipe au service de la goodtech pour notre projet de fin de bootcamp du wagon batch#1749 à Marseille 


Les contributeurs : 
- Benjamin qui s'est occupé de la mise en ligne et l'interface.

github : ChattanoogaChooChoo

- Célian qui s'est occupé du modèle dans son ensemble. 

github : celianbdt

- Hana qui s'est occupée du clean data et de la visualisation

github : Hanafk

- Dany, lead du projet, qui s'est occupé de la présentation et de la synchronisation de projet

github : Dany-Santi-TSAN

Sous la supervision de notre professeur Malika

github : MaLika67